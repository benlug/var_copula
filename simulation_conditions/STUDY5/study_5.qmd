---
title: "Study 5: SEM with Exponential Margins — Indicator vs. Latent Skewness Layer"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    code-tools: true
  pdf:
    toc: true
    toc-depth: 3
    documentclass: scrartcl
    classoption:
      - DIV=11
      - numbers=noendperiod
    header-includes:
      - \KOMAoption{captions}{tableheading}
execute:
  warning: false
  message: false
---

```{r setup}
suppressPackageStartupMessages({
  library(dplyr)
  library(tidyr)
  library(readr)
  library(ggplot2)
  library(stringr)
  library(forcats)
  library(knitr)
  library(scales)
})

# Default chunk behavior: render this as a report (not a code listing)
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  dpi = 300
)

# ---- Paths (robust to running from either STUDY5/ or repository root) ----
BASE_DIR <- "."
if (!dir.exists(file.path(BASE_DIR, "data")) && dir.exists("simulation_conditions")) {
  cand <- file.path("simulation_conditions", "STUDY5")
  if (dir.exists(cand)) BASE_DIR <- cand
}

DATA_DIR   <- file.path(BASE_DIR, "data")
RES_DIR    <- file.path(BASE_DIR, "results_sem")
EXPORT_DIR <- file.path(RES_DIR, "exported_tables")
dir.create(EXPORT_DIR, recursive = TRUE, showWarnings = FALSE)

files <- list(
  design = file.path(DATA_DIR, "sim_conditions_sem.rds"),
  rep    = file.path(RES_DIR,  "summary_replications_sem.csv"),
  cond   = file.path(RES_DIR,  "summary_conditions_sem.csv")
)

missing <- names(files)[!file.exists(unlist(files))]
if (length(missing) > 0) {
  cat("\n\n### Missing required inputs\n\n")
  cat("This report expects outputs created by the Study 5 pipeline.\n\n")
  cat("Run, in order:\n")
  cat("1. `run_pipeline_SEM.R` (simulate + fit)\n")
  cat("2. `analysis_sem.R` (create summary CSVs)\n\n")
  cat("Missing file(s):\n")
  for (nm in missing) cat(paste0("- `", files[[nm]], "`\n"))
  cat("\n")
  knitr::knit_exit()
}

# ---- Read inputs ----
design_raw <- readRDS(files$design)
rep_raw    <- read_csv(files$rep,  show_col_types = FALSE)
cond_raw   <- read_csv(files$cond, show_col_types = FALSE)

# ---- Consistent labels ----
T_levels <- sort(unique(design_raw$T))

design <- design_raw %>%
  mutate(
    sem_study = factor(
      sem_study,
      levels = c("A_indicator", "B_latent"),
      labels = c("Study A: indicator-skew", "Study B: latent-skew")
    ),
    direction = factor(direction, levels = c("++", "+-")),
    T = factor(T, levels = T_levels),
    rho = factor(rho)
  ) %>%
  select(condition_id, sem_study, direction, T, rho, n_reps)

rep_df <- rep_raw %>%
  filter(!is.na(param)) %>%
  left_join(design, by = "condition_id") %>%
  mutate(
    Model = factor(
      model,
      levels = c("EI", "EL"),
      labels = c("EI: Exponential at indicator", "EL: Exponential at latent")
    ),
    status_raw = status,
    status = factor(
      status,
      levels = c("ok", "divergent", "bad_rhat", "failed"),
      labels = c("ok", "divergent", "bad R-hat", "failed")
    ),
    n = as.integer(as.character(T))
  )

cond <- cond_raw %>%
  left_join(design, by = "condition_id") %>%
  mutate(
    Model = factor(
      model,
      levels = c("EI", "EL"),
      labels = c("EI: Exponential at indicator", "EL: Exponential at latent")
    ),
    n = as.integer(as.character(T)),
    sd_bias = mean_post_sd - emp_sd
  )

# ---- Plot styling ----
theme_study <- function() {
  theme_minimal(base_size = 12) +
    theme(
      legend.position = "bottom",
      panel.grid.minor = element_blank(),
      strip.text = element_text(face = "bold")
    )
}

# ---- Parameters of interest ----
core_params <- c(
  "mu[1]", "mu[2]",
  "phi[1,1]", "phi[1,2]", "phi[2,1]", "phi[2,2]",
  "rho"
)

sigma_params <- c("sigma_exp[1]", "sigma_exp[2]")

# Replication-level diagnostics (one row per fit)
rep_level <- rep_df %>%
  distinct(model, Model, condition_id, rep_id, sem_study, direction, T, n, rho,
           status_raw, status, n_div, max_rhat)

# Helper: safe percent formatter
pct <- function(x) percent(x, accuracy = 0.1)
```

## Purpose

This simulation study compares two SEM formulations that differ only in **where Exponential skewness is placed**:

- **EI (indicator-skew):** Exponential (signed/shifted) margins are applied to the *measurement errors*; the Gaussian copula dependence (parameter $\rho$) is also imposed at this layer.
- **EL (latent-skew):** Exponential (signed/shifted) margins are applied to the *state innovations*; the copula dependence is imposed at the latent-innovation layer.

The primary question is sensitivity: **How much do inference and recovery of the core dynamic parameters change when the skewness layer is mis-specified?**

We focus on recovery of:

- $\mu$ (state intercept)
- $\Phi$ (VAR(1) coefficient matrix)
- $\rho$ (copula correlation at the active layer)

and report the implied Exponential shift parameters $\sigma_{\mathrm{exp}}$ separately.

::: {.callout-important}
### Important interpretation note
Condition-level performance summaries in `summary_conditions_sem.csv` are computed **only over fits classified as `ok`** by `analysis_sem.R` (i.e., no divergences and $\widehat{R}\le 1.01$). This is useful for understanding *conditional* estimator behavior, but it can overstate performance if a large fraction of fits are problematic or fail.
:::

## Simulation design

```{r design-summary}
design_overview <- tibble(
  `SEM study` = paste(levels(design$sem_study), collapse = "; "),
  `Skew direction` = paste(levels(design$direction), collapse = ", "),
  `Time series length (T)` = paste(levels(design$T), collapse = ", "),
  `Copula correlation (rho)` = paste(levels(design$rho), collapse = ", "),
  `Replications / cell` = paste(sort(unique(design$n_reps)), collapse = ", "),
  `Total conditions` = n_distinct(design$condition_id)
)

kable(design_overview, align = "l")
```

### Data-generating process

Both studies use a 2-dimensional latent state $x_t$ with VAR(1) dynamics and two observed indicators $y_t$:

$$
\begin{aligned}
  x_t &= \mu + \Phi x_{t-1} + u_t, \\
  y_t &= x_t + \varepsilon_t.
\end{aligned}
$$

- In **Study A**, $u_t$ is Gaussian and the measurement errors $\varepsilon_t$ have signed/shifted Exponential margins with copula dependence.
- In **Study B**, $\varepsilon_t$ is Gaussian and the state innovations $u_t$ have signed/shifted Exponential margins with copula dependence.

The *skew direction* ("++" vs "+-") controls the sign pattern of the Exponential transform across the two dimensions.

## Estimation targets and summaries

- **Replication-level summaries** (`summary_replications_sem.csv`) include posterior means/SDs, 95% credible interval bounds, divergences, and $\widehat{R}$ diagnostics.
- **Condition-level summaries** (`summary_conditions_sem.csv`) aggregate performance across replications *after filtering to `ok` fits*.

For the bias plots below:

- Reported *relative bias* is $\mathrm{bias}/|\theta|$ when $|\theta|>0$.
- For parameters with true value $0$ (e.g., $\mu$ in this design), the reported “relative bias” reduces to *absolute bias*.

## Diagnostics and fit quality

### Status rates

```{r status-rates, fig.width=10, fig.height=4.8}
status_rates <- rep_level %>%
  count(Model, sem_study, T, direction, status, name = "n") %>%
  group_by(Model, sem_study, T, direction) %>%
  mutate(prop = n / sum(n)) %>%
  ungroup()

p_status <- ggplot(status_rates, aes(x = direction, y = prop, fill = status)) +
  geom_col(width = 0.85) +
  facet_grid(Model ~ sem_study + T) +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  labs(
    x = "Skew direction",
    y = "Proportion of replications",
    fill = "Fit status",
    title = "MCMC status by model, SEM study, sample size, and skew direction"
  ) +
  theme_study()

p_status
```

### Divergences

```{r divergences, fig.width=10, fig.height=4.5}
p_div <- ggplot(rep_level, aes(x = direction, y = n_div, color = Model)) +
  geom_boxplot(outlier.alpha = 0.4) +
  facet_grid(sem_study ~ T) +
  labs(
    x = "Skew direction",
    y = "Number of divergent transitions",
    color = "Model",
    title = "Divergences across replications"
  ) +
  theme_study()

p_div
```

### Maximum $\widehat{R}$

```{r rhat, fig.width=10, fig.height=4.5}
p_rhat <- ggplot(rep_level, aes(x = direction, y = max_rhat, color = Model)) +
  geom_boxplot(outlier.alpha = 0.4) +
  geom_hline(yintercept = 1.01, linetype = "dashed") +
  facet_grid(sem_study ~ T) +
  labs(
    x = "Skew direction",
    y = "Max R-hat across parameters",
    color = "Model",
    title = "Convergence diagnostics (threshold at 1.01)"
  ) +
  theme_study()

p_rhat
```

## Core parameter recovery

```{r core-data}
cond_core <- cond %>%
  filter(param %in% core_params, N_valid > 0) %>%
  mutate(
    param = factor(param, levels = core_params),
    direction = factor(direction, levels = c("++", "+-")),
    T = factor(T, levels = T_levels)
  )
```

### Relative bias (or absolute bias when truth is 0)

```{r core-bias, fig.width=10.5, fig.height=8.5}
p_bias <- ggplot(cond_core, aes(x = n, y = mean_rel_bias, color = Model, group = Model)) +
  geom_hline(yintercept = 0, linetype = "dotted") +
  geom_line() +
  geom_point(size = 1.6) +
  facet_grid(param ~ sem_study + direction) +
  scale_x_continuous(breaks = sort(unique(cond_core$n))) +
  labs(
    x = "Sample size (T)",
    y = "Relative bias",
    color = "Model",
    title = "Core parameters: bias as a function of sample size"
  ) +
  theme_study()

p_bias
```

### 95% interval coverage

```{r core-coverage, fig.width=10.5, fig.height=8.5}
p_cov <- ggplot(cond_core, aes(x = n, y = coverage_95, color = Model, group = Model)) +
  geom_hline(yintercept = 0.95, linetype = "dotted") +
  geom_line() +
  geom_point(size = 1.6) +
  facet_grid(param ~ sem_study + direction) +
  scale_x_continuous(breaks = sort(unique(cond_core$n))) +
  scale_y_continuous(limits = c(0, 1), labels = percent_format(accuracy = 1)) +
  labs(
    x = "Sample size (T)",
    y = "Coverage",
    color = "Model",
    title = "Core parameters: 95% credible-interval coverage"
  ) +
  theme_study()

p_cov
```

### Posterior SD calibration ($\overline{\mathrm{SD}}_{\mathrm{post}} - \mathrm{SD}(\overline{\theta})$)

```{r core-sd-bias, fig.width=10.5, fig.height=8.5}
p_sd <- ggplot(cond_core, aes(x = n, y = sd_bias, color = Model, group = Model)) +
  geom_hline(yintercept = 0, linetype = "dotted") +
  geom_line() +
  geom_point(size = 1.6) +
  facet_grid(param ~ sem_study + direction) +
  scale_x_continuous(breaks = sort(unique(cond_core$n))) +
  labs(
    x = "Sample size (T)",
    y = "SD bias",
    color = "Model",
    title = "Core parameters: posterior SD calibration"
  ) +
  theme_study()

p_sd
```


## Sensitivity to diagnostic filtering

Condition-level summaries (`summary_conditions_sem.csv`) are computed **only from replications flagged as `ok`** in `analysis_sem.R` (no divergences and $\widehat{R} \le 1.01$). This is typically the right choice for reporting inferential quality, but it is also useful to understand how much performance deteriorates when including replications with diagnostics warnings.

The table below focuses on two high-leverage quantities: the cross-lag $\phi_{1,2}$ and the copula correlation $\rho$.

```{r diag_sensitivity}
focus_params <- c("phi[1,2]", "rho")

sens_tbl <- rep_df %>%
  filter(param %in% focus_params, !is.na(bias), !is.na(cover95)) %>%
  group_by(Model, sem_study, param, status_raw) %>%
  summarise(
    N = n(),
    mean_bias = mean(bias, na.rm = TRUE),
    mean_abs_bias = mean(abs(bias), na.rm = TRUE),
    coverage_95 = mean(cover95, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    param = factor(param, levels = focus_params),
    status_raw = factor(status_raw, levels = c("ok", "divergent", "bad_rhat", "failed"))
  ) %>%
  arrange(param, sem_study, Model, status_raw)

kable(
  sens_tbl,
  digits = 3,
  caption = "Replication-level bias and coverage by diagnostic status (selected parameters)."
)
```

```{r diag_sensitivity_plot, fig.width=10, fig.height=4}
sens_plot <- sens_tbl %>%
  filter(!is.na(coverage_95), status_raw != "failed")

if (nrow(sens_plot) > 0) {
  ggplot(sens_plot, aes(x = status_raw, y = coverage_95, color = Model, group = Model)) +
    geom_point(size = 2) +
    geom_line() +
    facet_grid(param ~ sem_study) +
    geom_hline(yintercept = 0.95, linetype = "dashed") +
    scale_y_continuous(limits = c(0, 1), labels = percent_format(accuracy = 1)) +
    labs(x = NULL, y = "Coverage (95% interval)") +
    theme_study()
}
```


## Exponential shift parameters $\sigma_{\mathrm{exp}}$

::: {.callout-note}
### Why $\sigma_{\mathrm{exp}}$ needs care
In these Stan implementations, $\sigma_{\mathrm{exp}}$ is an *implied shift* ensuring positivity of the Exponential likelihood after applying the signed residual transform. It depends on sample extremes through a (smoothed) maximum term. As a result, treating the “truth” as exactly 1 (the shift used in simulation) is a **reference point**, not a strict population parameter in the same way as $\mu$, $\Phi$, or $\rho$.
:::

```{r sigma-data}
cond_sigma <- cond %>%
  filter(param %in% sigma_params, N_valid > 0) %>%
  mutate(
    param = factor(param, levels = sigma_params),
    direction = factor(direction, levels = c("++", "+-")),
    T = factor(T, levels = T_levels)
  )
```

### Bias

```{r sigma-bias, fig.width=10, fig.height=4.8}
p_sig_bias <- ggplot(cond_sigma, aes(x = n, y = mean_bias, color = Model, group = Model)) +
  geom_hline(yintercept = 0, linetype = "dotted") +
  geom_line() +
  geom_point(size = 1.6) +
  facet_grid(param ~ sem_study + direction) +
  scale_x_continuous(breaks = sort(unique(cond_sigma$n))) +
  labs(
    x = "Sample size (T)",
    y = "Bias",
    color = "Model",
    title = "Implied Exponential shift parameters: bias (reference truth = 1)"
  ) +
  theme_study()

p_sig_bias
```

### Coverage

```{r sigma-coverage, fig.width=10, fig.height=4.8}
p_sig_cov <- ggplot(cond_sigma, aes(x = n, y = coverage_95, color = Model, group = Model)) +
  geom_hline(yintercept = 0.95, linetype = "dotted") +
  geom_line() +
  geom_point(size = 1.6) +
  facet_grid(param ~ sem_study + direction) +
  scale_x_continuous(breaks = sort(unique(cond_sigma$n))) +
  scale_y_continuous(limits = c(0, 1), labels = percent_format(accuracy = 1)) +
  labs(
    x = "Sample size (T)",
    y = "Coverage",
    color = "Model",
    title = "Implied Exponential shift parameters: 95% interval coverage"
  ) +
  theme_study()

p_sig_cov
```

## Quick cross-condition summaries

```{r summaries}
# Diagnostics summary
status_summary <- rep_level %>%
  count(Model, sem_study, status_raw, name = "n") %>%
  group_by(Model, sem_study) %>%
  mutate(prop = n / sum(n)) %>%
  ungroup() %>%
  arrange(sem_study, Model, desc(prop))

# Core performance summary (conditional on ok fits, because cond is built that way)
core_summary <- cond_core %>%
  group_by(Model, sem_study, param) %>%
  summarise(
    avg_abs_rel_bias = mean(abs(mean_rel_bias), na.rm = TRUE),
    avg_coverage = mean(coverage_95, na.rm = TRUE),
    avg_RMSE = mean(RMSE, na.rm = TRUE),
    .groups = "drop"
  )

cat("### Diagnostic status proportions (replication-level)\n\n")
kable(
  status_summary %>%
    mutate(prop = percent(prop, accuracy = 0.1)) %>%
    select(Model, sem_study, status = status_raw, n, prop),
  align = "l"
)

cat("\n\n### Average core-parameter performance (condition-level; ok fits only)\n\n")
kable(
  core_summary %>%
    mutate(
      avg_coverage = percent(avg_coverage, accuracy = 0.1)
    ),
  digits = 3,
  align = "l"
)
```

## Exportable tables

```{r export}
export_core <- cond_core %>%
  select(
    sem_study, direction, T, n, rho,
    Model, param,
    N_valid, mean_bias, mean_rel_bias,
    RMSE, coverage_95,
    mean_post_sd, emp_sd, sd_bias,
    mean_rhat
  ) %>%
  arrange(sem_study, direction, n, Model, param)

write_csv(export_core, file.path(EXPORT_DIR, "study5_core_summary.csv"))

export_sigma <- cond_sigma %>%
  select(
    sem_study, direction, T, n, rho,
    Model, param,
    N_valid, mean_bias, mean_rel_bias,
    RMSE, coverage_95,
    mean_post_sd, emp_sd, sd_bias,
    mean_rhat
  ) %>%
  arrange(sem_study, direction, n, Model, param)

write_csv(export_sigma, file.path(EXPORT_DIR, "study5_sigma_summary.csv"))

cat("Tables written to:\n")
cat(paste0("- ", file.path(EXPORT_DIR, "study5_core_summary.csv"), "\n"))
cat(paste0("- ", file.path(EXPORT_DIR, "study5_sigma_summary.csv"), "\n"))
```

## Notes on interpretation

1. **Layer mis-specification affects identification.** In this setup, $\rho$ is identified at the *active layer*; fitting the wrong layer can distort both dependence and dynamic parameters, not just the marginal residual behavior.

2. **Condition-level summaries are conditional.** If a model has low `ok` rates for certain conditions, its condition-level bias/coverage plots may look “fine” simply because only the best-behaved runs are retained.

3. **The Exponential shift parameters are not pure scale parameters.** The $\sigma_{\mathrm{exp}}$ quantities are entangled with sample extremes via a maximum term. Treat comparisons to the simulation shift of 1 as diagnostic rather than as a primary target.
