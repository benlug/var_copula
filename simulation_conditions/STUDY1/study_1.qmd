---
title: "Study 1: Analysis"
author: "Benedikt Lugauer"
format:
  html:
    toc: true
    toc_depth: 3
    code-fold: true
    theme: lumen
    self-contained: true
  pdf:
    toc: true
    toc_depth: 3
    # Ensure you have a LaTeX distribution installed (e.g., TinyTeX)
    latex_engine: xelatex
execute:
  warning: false
  message: false
---

```{r setup}
#| label: setup

# Load necessary libraries
suppressPackageStartupMessages({
  library(dplyr)
  library(tidyr)
  library(readr)
  library(ggplot2)
  library(stringr)
  library(knitr)
  library(RColorBrewer)
  library(ggh4x) # Required for nested facets if needed
})


# Define paths (assuming this notebook is in the main project directory)
# If running interactively, ensure the working directory is set to the project root.
# BASE_DIR <- getwd()
DATA_DIR <- file.path("data")
RES_DIR <- file.path("results")
EXPORT_DIR <- file.path(RES_DIR, "exported_tables")
dir.create(EXPORT_DIR, showWarnings = FALSE, recursive = TRUE)

files <- list(
  cond    = file.path(RES_DIR, "summary_conditions.csv"),
  rep     = file.path(RES_DIR, "summary_replications.csv"),
  design = file.path(DATA_DIR, "sim_conditions_singlelevel.rds")
)

# Check if files exist
if (!all(file.exists(unlist(files)))) {
    stop("Missing required input files. Please ensure analysis_singlelevel.R has been run.")
}
```

## 1\. Introduction

This simulation study compares the performance of two Bayesian Vector Autoregressive (VAR(1)) models: a standard Normal-Gaussian (NG) model and a flexible Skew-Normal-Gaussian (SG) model. The study investigates how these models recover the true parameters when the data generating process (DGP) exhibits varying degrees of skewness in the innovations, coupled by a Gaussian copula.

### 1.1. Data Generating Process (DGP)

The underlying DGP is a bivariate VAR(1) model:

$$Y_t = \mu + \Phi Y_{t-1} + \epsilon_t$$

Where $Y\_t$ is a 2x1 vector of observations, $\\mu$ is the intercept vector (set to 0 in this simulation), $\\Phi$ is the 2x2 matrix of autoregressive coefficients, and $\\epsilon\_t$ is the 2x1 vector of innovations (errors).

The innovations $\\epsilon\_t = (\\epsilon\_{1,t}, \\epsilon\_{2,t})^T$ are generated such that they are standardized (Mean=0, Variance=1). The joint distribution of $\\epsilon\_t$ is modeled using a **Gaussian Copula**, parameterized by the correlation coefficient $\\rho$. This allows the dependence structure to be modeled independently of the marginal distributions $f\_i(\\epsilon\_{i,t})$. The joint density is given by Sklar's theorem:

$$f(\epsilon_{1,t}, \epsilon_{2,t}) = c(u_{1,t}, u_{2,t}; \rho) \cdot f_1(\epsilon_{1,t}) \cdot f_2(\epsilon_{2,t})$$

The marginal distributions $f\_i$ are varied to introduce different levels and types of skewness.

### 1.2. Simulation Design

The study employs a full factorial design crossing five factors, resulting in 108 unique conditions, with 200 replications per condition.

```{r design_table}
#| label: design_table
#| echo: false

# Define the levels for the table manually for clarity
# NOTE: Double backslashes (\\) are required here because this is inside an R string 
# that will be interpreted as LaTeX by kable(escape=FALSE).
design_summary <- tibble(
  Factor = c("Time Series Length (T)",
             "Copula Correlation ($\\rho$)",
             "VAR Parameters ($\\Phi$)",
             "",
             "Skewness Level (Marginals)",
             "",
             "",
             "Skewness Direction"),
  # For the matrix newline, we need \\\\ to produce a literal \\ in the output markdown.
  Levels = c("50, 100, 200",
             "0.30, 0.50",
             "**Set A** (Symmetric): $\\begin{pmatrix} 0.40 & 0.10 \\\\ 0.10 & 0.40 \\end{pmatrix}$",
             "**Set B** (Asymmetric): $\\begin{pmatrix} 0.55 & 0.10 \\\\ 0.10 & 0.25 \\end{pmatrix}$",
             "`moderateSN`: Skew-Normal (SN), shape $\\alpha = \\pm 4$",
             "`strongSN`: SN, shape $\\alpha = \\pm 9$",
             "`extremeCHI`: Standardized Chi-squared ($\\chi^2_1$)",
             "`++` (Both positive), `--` (Both negative), `+-` (Mixed)")
)

kable(design_summary, caption = "Summary of the Simulation Design Factors.", escape = FALSE)
```

## 2\. Data Loading and Preparation

```{r load_data}
#| label: load_data

# Load the design grid
design <- readRDS(files$design) |>
  select(condition_id, skew_level, direction, T, rho, VARset)

# Load condition-level summary (aggregated metrics)
cond_raw <- read_csv(files$cond, show_col_types = FALSE) |>
  left_join(design, by = "condition_id")

# Load replication-level summary (individual runs)
rep_raw <- read_csv(files$rep, show_col_types = FALSE) |>
    filter(!is.na(param)) |> # Filter out completely failed fits
    left_join(design, by = "condition_id")


# Define parameter order and groups
param_levels <- c("omega[1]","omega[2]","alpha[1]","alpha[2]",
                  "sigma[1]","sigma[2]",
                  "mu[1]","mu[2]",
                  "phi11","phi12","phi21","phi22","rho")

# Apply factor levels and clearer labels
prep_data <- function(df) {
  df |>
    mutate(param = factor(param, levels = param_levels),
           T = factor(T),
           # Ensure informative labels for facets and correct ordering
           skew_level = factor(skew_level, levels = c("moderateSN", "strongSN", "extremeCHI")),
           rho_val = rho, # Keep numeric rho
           VARset_val = VARset, # Keep character VARset
           # MODIFIED: Removed manual prefixes (e.g., "VARset:"). 
           # We rely on ggplot's labeller=label_both used later.
           rho = factor(rho, labels = sort(unique(df$rho))),
           VARset = factor(VARset, labels = sort(unique(df$VARset))),
           Model = factor(ifelse(model == "SG", "Skew-Normal (SG)", "Normal (NG)"),
                          levels = c("Normal (NG)", "Skew-Normal (SG)")))
}

cond <- prep_data(cond_raw)
rep_df <- prep_data(rep_raw)


# Calculate Root Mean Squared Error (RMSE)
# RMSE = sqrt(Bias^2 + Empirical_SD^2) - provides overall accuracy measure
cond <- cond |>
  mutate(
    # Use coalesce to handle NA emp_sd if N_valid < 2
    RMSE = sqrt(mean_bias^2 + coalesce(emp_sd^2, 0))
  )


# Separate core parameters (VAR dynamics, intercepts, correlation)
core_params <- c("mu[1]","mu[2]", "phi11","phi12","phi21","phi22","rho")
```

### 2.1. MCMC Classification and Overview

We classify runs based on MCMC diagnostics (R-hat and divergent transitions `n_div`) and summarize the computational performance.

```{r classify_mcmc}
#| label: classify_mcmc

RHAT_THRESHOLD <- 1.01

rep_df <- rep_df |>
  mutate(
    mcmc_status = case_when(
      is.na(max_rhat) | status != "ok" ~ "Failed/Error",
      # Classification based on Rhat and divergent transitions (n_div)
      max_rhat > RHAT_THRESHOLD | n_div > 0 ~ "Problematic",
      TRUE ~ "Clean"
    ),
    mcmc_status = factor(mcmc_status, levels = c("Clean", "Problematic", "Failed/Error"))
  )

# MCMC Overview Plot (Status Counts)
mcmc_summary <- rep_df |>
  distinct(condition_id, rep_id, Model, mcmc_status, T, skew_level) |>
  group_by(Model, T, skew_level, mcmc_status) |>
  summarise(Count = n(), .groups = "drop")
```

```{r mcmc_status_plot, fig.height=12, fig.width=16}
#| label: mcmc_status_plot
ggplot(mcmc_summary, aes(x = T, y = Count, fill = mcmc_status)) +
  geom_bar(stat = "identity", position = "stack") +
  facet_grid(Model ~ skew_level) +
  labs(x = "Time Series Length (T)", y = "Number of Replications", fill = "MCMC Status",
       title = "MCMC Convergence Status by Condition") +
  theme_bw(base_size = 14) +
  scale_fill_manual(values = c("Clean" = "#4daf4a", "Problematic" = "#ff7f00", "Failed/Error" = "#e41a1c"))
```

**Interpretation: MCMC Status Counts**
The computational performance shows a stark contrast between the models. The NG model is extremely stable, achieving a "Clean" MCMC status in virtually 100% of replications across all conditions. This stability holds even when the model is statistically misspecified. In contrast, the SG model exhibits significant computational fragility that increases with the skewness of the Data Generating Process (DGP). Under `moderateSN`, the SG model performs reasonably well, but under `strongSN`, a substantial portion of runs become "Problematic," especially at T=50. The `extremeCHI` condition (Chi-Squared DGP) causes severe difficulties for the SG model, resulting in nearly 100% "Problematic" runs, likely due to the difficulty of approximating the Chi-Squared DGP (model misspecification) and the resulting complex posterior geometry.

```{r divergence_overview, fig.height=9, fig.width=16}
#| label: divergence_overview

# MCMC Overview Plot (Distribution of Divergent Transitions)
# We use the replication level data (rep_df) to see the distribution of n_div across individual runs.

div_dist_data <- rep_df |>
    # Ensure we only look at the diagnostics once per replication run (n_div is the same for all params in a run)
    # We filter by one parameter (e.g., rho) to get unique runs, ensuring we have the n_div value for that run.
    filter(param == "rho") |>
    distinct(condition_id, rep_id, Model, T, skew_level, n_div, mcmc_status) |>
    # Exclude runs that failed entirely before sampling (if any)
    filter(mcmc_status != "Failed/Error")

ggplot(div_dist_data, aes(x = T, y = n_div, fill = Model)) +
  # Boxplot showing the distribution of divergences per condition
  # We hide default outliers as we will show them with jitter
  geom_boxplot(outlier.shape = NA, alpha = 0.6, position = position_dodge(width = 0.8)) +
  # Jitter points to show individual runs; this helps visualize the density and outliers
  # position_jitterdodge allows jittering within the dodged groups (Models)
  geom_point(size = 1.5, alpha = 0.4, position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.8)) +
  facet_grid(Model ~ skew_level) +
  theme_bw(base_size = 14) +
  # Note: A log1p scale (log(y+1)) might be necessary if the distribution is highly skewed with very large outliers
  # scale_y_continuous(trans = 'log1p', breaks = c(0, 5, 50, 200)) + 
  labs(title = "Distribution of Divergent Transitions (Post-Warmup) per Replication",
       y = "Count of Divergences (n_div)",
       x = "Time Points (T)")
```

**Interpretation: Divergent Transitions**
The boxplots confirm the pattern seen in the status counts. The NG model exhibits virtually zero divergent transitions across all conditions. The SG model, however, shows a significant number of divergences, particularly under `strongSN` and `extremeCHI`. The distribution is highly skewed, with many runs having few divergences, but a substantial number having dozens or hundreds, indicating severe sampling difficulties in those specific replications.

## 3\. Analysis Helpers

To systematically analyze the results, we define helper functions for the standard plots (Bias, Coverage, SD-Bias, RMSE, Posterior SD).

```{r analysis_helpers}
#| label: analysis_helpers

# Standardized visualization settings
theme_standard <- theme_bw(base_size = 14) # Increase base font size
dodge_width <- 0.3

# Helper function for plotting metrics across conditions
plot_metric <- function(data, metric_col, ylab, title, use_free_y = FALSE, ylims = NULL) {
  
  # Filter out potential NAs (e.g. if N_truth_avail = 0)
  data_filtered <- data |> filter(!is.na(.data[[metric_col]]))
  
  if (nrow(data_filtered) == 0) {
      message("Skipping plot '", title, "' due to missing data.")
      return(NULL)
  }
  
  p <- ggplot(data_filtered, aes(x = T, y = .data[[metric_col]], color = Model, group = Model)) +
    geom_line(position = position_dodge(dodge_width), linewidth = 1) +
    geom_point(position = position_dodge(dodge_width), size = 2.5) +
    # labeller = label_both correctly adds the variable name (VARset, rho) and the value.
    facet_grid(param ~ direction + VARset + rho, labeller = label_both, scales = ifelse(use_free_y, "free_y", "fixed")) +
    theme_standard +
    labs(title = title, y = ylab, x = "Time Points (T)")
  
  # Add reference lines based on the metric
  if (metric_col %in% c("mean_rel_bias", "sd_bias")) {
    p <- p + geom_hline(yintercept = 0, linetype = "dashed", color = "darkgrey")
  } else if (metric_col == "coverage_95") {
    p <- p + geom_hline(yintercept = 0.95, linetype = "dashed", color = "darkgrey")
  }
  
  # Apply custom Y-axis limits if provided
  if (!is.null(ylims)) {
    p <- p + coord_cartesian(ylim = ylims)
  }
  
  return(p)
}

# Wrapper to filter data and call the plotting function for a specific skew level
generate_plots_for_condition <- function(skew_lvl) {
  data_subset <- cond |>
    filter(skew_level == skew_lvl, param %in% core_params)
  
  # Adjust Y-axis limits for coverage based on the condition severity
  cov_ylims <- if (skew_lvl == "extremeCHI") c(0.5, 1.0) else c(0.8, 1.0)
  
  list(
    # Metric: Relative Bias
    bias = plot_metric(data_subset, "mean_rel_bias", "Mean Relative Bias", 
                       paste("Relative Bias (DGP:", skew_lvl, ")"), use_free_y = TRUE),
    # Metric: 95% CI Coverage
    coverage = plot_metric(data_subset, "coverage_95", "Empirical Coverage", 
                           paste("95% Coverage (DGP:", skew_lvl, ")"), ylims = cov_ylims),
    # Metric: RMSE (Overall Accuracy)
    rmse = plot_metric(data_subset, "RMSE", "Root Mean Squared Error", 
                       paste("RMSE (DGP:", skew_lvl, ")"), use_free_y = TRUE),
    # Metric: Posterior SD (Uncertainty Estimate)
    post_sd = plot_metric(data_subset, "mean_post_sd", "Mean Posterior SD", 
                          paste("Mean Posterior SD (DGP:", skew_lvl, ")"), use_free_y = TRUE),
    # Metric: SD-Bias (Calibration of Uncertainty)
    sdbias = plot_metric(data_subset, "sd_bias", "SD-Bias (Negative=Overconfident)", 
                         paste("SD-Bias (DGP:", skew_lvl, ")"), use_free_y = TRUE)
  )
}

```

## 4\. Condition 1: Moderate Skewness (moderateSN)

DGP: Skew-Normal innovations ($\\alpha=4$). NG model is misspecified; SG model is correctly specified.

```{r moderateSN_plots, results="hide"}
#| label: moderateSN_plots

plots_mod <- generate_plots_for_condition("moderateSN")
```

### 4.1. Relative Bias (moderateSN)

```{r moderateSN_bias, fig.height=12, fig.width=16}
#| label: moderateSN_bias
print(plots_mod$bias)
```

### 4.2. 95% Coverage (moderateSN)

```{r moderateSN_coverage, fig.height=12, fig.width=16}
#| label: moderateSN_coverage
print(plots_mod$coverage)
```

### 4.3. SD-Bias (moderateSN)

```{r moderateSN_sdbias, fig.height=12, fig.width=16}
#| label: moderateSN_sdbias
# Note: RMSE and Mean Posterior SD plots are omitted for brevity, focusing on Bias, Coverage, and SD-Bias.
print(plots_mod$sdbias)
```

**Interpretation (moderateSN):**
Under moderate skewness, both models perform reasonably well. The SG model is accurate, with bias near zero and coverage near the nominal 0.95 rate. The NG model shows slight biases, particularly a minor underestimation of $\\rho$ and some attenuation (bias toward zero) in the $\\Phi$ parameters. The NG model also exhibits slight negative SD-Bias (overconfidence) leading to minor undercoverage for $\\rho$. Overall, the NG model appears relatively robust to this moderate departure from normality.

## 5\. Condition 2: Strong Skewness (strongSN)

DGP: Skew-Normal innovations ($\\alpha=9$). The misspecification for the NG model is severe.

```{r strongSN_plots, results="hide"}
#| label: strongSN_plots
plots_strong <- generate_plots_for_condition("strongSN")
```

### 5.1. Relative Bias (strongSN)

```{r strongSN_bias, fig.height=12, fig.width=16}
#| label: strongSN_bias
print(plots_strong$bias)
```

### 5.2. 95% Coverage (strongSN)

```{r strongSN_coverage, fig.height=12, fig.width=16}
#| label: strongSN_coverage
print(plots_strong$coverage)
```

### 5.3. SD-Bias (strongSN)

```{r strongSN_sdbias, fig.height=12, fig.width=16}
#| label: strongSN_sdbias
print(plots_strong$sdbias)
```

**Interpretation (strongSN):**
The performance gap widens significantly. The NG model's biases become substantial. The underestimation of $\\rho$ and the attenuation of $\\Phi$ are more pronounced than in the moderate condition. The SG model remains accurate and well-calibrated. The NG model's SD-Bias is clearly negative, confirming systematic overconfidence, which explains why its coverage drops noticeably below 0.95, especially for the dependence parameter $\\rho$.

## 6\. Condition 3: Extreme Skewness and Misspecification (extremeCHI)

DGP: Standardized Chi-Squared (df=1) innovations. Highly skewed. Both models are misspecified, but SG is more flexible.

```{r extremeCHI_plots, results="hide"}
#| label: extremeCHI_plots
plots_extreme <- generate_plots_for_condition("extremeCHI")
```

### 6.1. Relative Bias (extremeCHI)

```{r extremeCHI_bias, fig.height=12, fig.width=16}
#| label: extremeCHI_bias
print(plots_extreme$bias)
```

### 6.2. 95% Coverage (extremeCHI)

```{r extremeCHI_coverage, fig.height=12, fig.width=16}
#| label: extremeCHI_coverage
print(plots_extreme$coverage)
```

### 6.3. SD-Bias (extremeCHI)

```{r extremeCHI_sdbias, fig.height=12, fig.width=16}
#| label: extremeCHI_sdbias
print(plots_extreme$sdbias)
```

**Interpretation (extremeCHI):**
The NG model fails catastrophically. The relative bias for $\\rho$ is extreme (often exceeding -1.0), indicating the model estimates the correlation to be near zero or even negative, despite the true positive correlation. Attenuation bias in $\\Phi$ is severe. Consequently, coverage is very poor.

The SG model demonstrates remarkable resilience in estimating the dynamics ($\\Phi$) and dependence ($\\rho$), maintaining low bias and good coverage for these parameters despite being technically misspecified and facing severe MCMC issues.

#### The Paradoxical Bias in the Intercept ($\\mu$)

A striking observation in the `extremeCHI` condition is the substantial bias in the intercepts ($\\mu$) for the SG model, while the NG model remains virtually unbiased for $\\mu$. This seems paradoxical, as the SG model is generally superior.

  * **Why NG is unbiased for $\\mu$:** The NG model assumes symmetric (Normal) innovations. Estimators for the intercept are generally robust to distributional violations as long as the errors have a mean of zero (which they do). The NG model cannot improve the fit by shifting $\\mu$; instead, it distorts $\\Phi$ and $\\rho$.
  * **Why SG is biased for $\\mu$:** This bias stems from the **Centered Parameterization (CP)** used in the SG model implementation. The CP stabilizes MCMC but imposes a strict constraint: the estimated Skew-Normal innovations *must* have a theoretical mean of zero. When the SG model tries to fit the highly irregular Chi-Squared data, it struggles. The Skew-Normal shape is a poor fit for the Chi-Squared shape. To reconcile the data with this poorly fitting shape *and* the zero-mean constraint, the model compensates by shifting the intercept $\\mu$. By biasing $\\mu$ (e.g., positively), the calculated residuals ($\\epsilon\_t = Y\_t - \\text{prediction}$) are shifted (e.g., negatively), allowing them to achieve a higher likelihood under the constrained Skew-Normal distribution.

In essence, the SG model sacrifices the intercept ($\\mu$) to compensate for the imperfect fit between the assumed and true shapes under the constraints of the CP, thereby protecting the accuracy of the dynamics ($\\Phi$) and dependence ($\\rho$).

## 7\. Cross-Condition Synthesis: Marginal Parameters

We examine the parameters governing the marginal distributions to understand the mechanisms driving the results.

### 7.1. The Mechanism of Failure: NG Variance Inflation ($\\sigma$)

The simulated data is standardized (Variance=1). The NG model should recover $\\sigma=1$.

```{r ng_sigma_inflation, fig.height=12, fig.width=16}
#| label: ng_sigma_inflation

sigma_data <- cond |>
  filter(param %in% c("sigma[1]", "sigma[2]"), Model == "Normal (NG)")

# We plot the absolute bias (mean_bias) here as the truth is 1.
ggplot(sigma_data, aes(x = T, y = mean_bias, color = skew_level, group = skew_level)) +
  geom_line(position = position_dodge(0.3), linewidth = 1) +
  geom_point(position = position_dodge(0.3), size = 2.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "darkgrey") +
  facet_grid(param ~ direction + VARset + rho, labeller = label_both) +
  theme_standard +
  labs(title = "NG Model: Bias for Sigma (Truth=1) - Variance Inflation/Deflation",
       y = "Mean Bias (Estimate - 1)",
       x = "Time Points (T)",
       color = "DGP Skew Level")
```

**Interpretation: Variance Inflation/Deflation**
The NG model's estimation of $\\sigma$ reveals how it handles misspecification. In the Skew-Normal conditions (`moderateSN`, `strongSN`), the NG model tends to slightly *overestimate* $\\sigma$ (positive bias). It misinterprets some of the skewness as excess variance. However, in the `extremeCHI` condition, the bias is significantly negative (variance deflation). Regardless of the direction, the misspecification of the marginal distribution leads to incorrect estimates of variance, which is a primary mechanism contributing to the attenuation bias observed in the $\\Phi$ parameters (the model compensates for incorrect innovation variance by adjusting the dynamics).

### 7.2. The Mechanism of Success: SG Shape Recovery ($\\alpha$)

We examine how well the SG model recovers the true shape parameter $\\alpha$ (applicable for SN conditions).

```{r sg_alpha_recovery, fig.height=12, fig.width=16}
#| label: sg_alpha_recovery

alpha_data <- cond |>
  filter(param %in% c("alpha[1]", "alpha[2]"),
         Model == "Skew-Normal (SG)",
         skew_level %in% c("moderateSN", "strongSN"))

# Filter NAs in case truth was unavailable
alpha_data <- alpha_data |> filter(!is.na(mean_rel_bias))

if (nrow(alpha_data) > 0) {
    ggplot(alpha_data, aes(x = T, y = mean_rel_bias, color = skew_level, group = skew_level)) +
      geom_line(position = position_dodge(0.3), linewidth = 1) +
      geom_point(position = position_dodge(0.3), size = 2.5) +
      geom_hline(yintercept = 0, linetype = "dashed", color = "darkgrey") +
      ggh4x::facet_nested(param ~ direction + VARset + rho, labeller = label_both, scales = "free_y") +
      theme_standard +
      labs(title = "SG Model: Relative Bias for Alpha (Shape Parameter)",
           y = "Relative Bias",
           x = "Time Points (T)",
           color = "DGP Skew Level")
}
```

**Interpretation: Shape Recovery**
The SG model shows substantial bias in recovering the shape parameters ($\\alpha$) at low T (T=50), often underestimating the magnitude of the skewness. However, the bias rapidly diminishes as T increases. Despite the difficulty in precisely estimating $\\alpha$, the mere inclusion of the shape parameter allows the SG model to capture the non-Gaussian features sufficiently to avoid the severe biases in $\\Phi$ and $\\rho$ seen in the NG model.

## 8\. Impact of MCMC Diagnostics

The SG model frequently encountered "Problematic" MCMC runs. We investigate if the statistical performance differs between "Clean" and "Problematic" runs for the SG model.

```{r reaggregate_by_status}
#| label: reaggregate_by_status

# Helper function to re-aggregate metrics, filtering by MCMC status
# It is crucial to recalculate Empirical SD, SD-Bias, and RMSE within the subgroups.
aggregate_by_status <- function(df) {
  df |>
    # Filter out total failures
    filter(mcmc_status != "Failed/Error") |>
    group_by(condition_id, Model, param, mcmc_status, T, skew_level, direction, VARset, rho, VARset_val, rho_val) |>
    summarise(
      N_valid = n(),
      mean_rel_bias = mean(rel_bias, na.rm = TRUE),
      coverage_95 = mean(cover95, na.rm = TRUE),
      # Recalculate components for SD-Bias and RMSE within the status group
      mean_post_sd = mean(post_sd, na.rm = TRUE),
      emp_sd = sd(post_mean, na.rm = TRUE),
      mean_bias = mean(bias, na.rm=TRUE),
      .groups = "drop"
    ) |>
    mutate(
        # Handle cases where N_valid=1, leading to NA emp_sd
        emp_sd = ifelse(is.na(emp_sd), 0, emp_sd),
        sd_bias = mean_post_sd - emp_sd,
        RMSE = sqrt(mean_bias^2 + emp_sd^2)
    )
}

cond_status <- aggregate_by_status(rep_df)
```

### 8.1. Coverage Split by MCMC Status (SG Model)

We visualize the coverage across all three conditions, comparing Clean vs. Problematic runs.

```{r coverage_status_split, fig.height=12, fig.width=16}
#| label: coverage_status_split

status_comparison_data <- cond_status |>
  filter(Model == "Skew-Normal (SG)",
         param %in% core_params)

# We focus the visualization on the interaction between T, Status, and Skew Level
# We average over VARset, rho, and direction for a clearer overview
status_overview <- status_comparison_data |>
  group_by(T, param, skew_level, mcmc_status) |>
  summarise(mean_coverage = mean(coverage_95, na.rm = TRUE), .groups = 'drop')


ggplot(status_overview,
       aes(x = T, y = mean_coverage, color = mcmc_status, group = mcmc_status)) +
  geom_line(position = position_dodge(0.3), linewidth = 1) +
  geom_point(position = position_dodge(0.3), size = 2.5) +
  geom_hline(yintercept = 0.95, linetype = "dashed", color = "darkgrey") +
  # Facet by parameter and skew level
  facet_grid(param ~ skew_level) +
  theme_standard +
  labs(title = "SG Model Coverage: Clean vs. Problematic Runs Across Conditions",
       y = "Average Empirical Coverage",
       x = "Time Points (T)",
       color = "MCMC Status") +
  coord_cartesian(ylim = c(0.8, 1.0))
```

**Interpretation: Impact of MCMC Status**
A crucial insight is the robustness of the SG model's inference to its MCMC issues. The statistical performance (coverage) of the SG model is remarkably similar between "Clean" and "Problematic" runs across all conditions. Even in the `extremeCHI` condition, where nearly all runs were problematic, coverage remains close to the nominal rate for the dynamic parameters (though coverage for $\\mu$ is poor due to the bias discussed in Section 6). This suggests that while the sampler encountered difficulties (e.g., divergences), the resulting posterior draws still provided valid statistical inference for the core VAR dynamics in this study.

### 8.2. Relationship between Bias and Divergences

We examine if runs with more divergences exhibit higher bias at the replication level. We focus on the SG model under `strongSN` conditions.

```{r bias_vs_divergences, fig.height=12, fig.width=16}
#| label: bias_vs_divergences

div_bias_data <- rep_df |>
  filter(Model == "Skew-Normal (SG)",
         skew_level == "strongSN",
         param %in% core_params,
         mcmc_status != "Failed/Error")

# Use absolute bias for comparison
ggplot(div_bias_data, aes(x = n_div, y = abs(bias))) +
  geom_point(alpha = 0.3, position = position_jitter(width = 0.2), size = 2) +
  # Use a generalized additive model (gam) for smoothing to capture non-linear relationships
  geom_smooth(method = "gam", color = "red", linewidth = 1.5) +
  facet_grid(param ~ T, scales = "free") +
  theme_standard +
  labs(title = "Absolute Bias vs. Divergences (SG Model, strongSN)",
       x = "Number of Divergent Transitions (n_div)",
       y = "Absolute Bias")

```

**Interpretation: Bias vs. Divergences**
There does not appear to be a strong correlation between the number of divergences and the absolute bias for the core parameters. High bias occurs in runs with few divergences, and low bias occurs in runs with many divergences. This reinforces the conclusion that MCMC diagnostics, while important indicators of computational issues, are not reliable predictors of statistical accuracy in this context.

## 9\. Conclusion

The simulation study strongly supports the use of the Skew-Normal Gaussian Copula (SG) model when analyzing time series data suspected of having non-Gaussian innovations.

1.  **NG Model Failure:** Relying on the standard Gaussian (NG) model in the presence of skewness leads to severe and systematic distortions, particularly the underestimation of dependence ($\\rho$) and the attenuation of dynamics ($\\Phi$).
2.  **SG Model Superiority:** The SG model consistently provides superior statistical inference, lower bias, and better uncertainty calibration for the dynamic and dependence parameters.
3.  **The Intercept Trade-off:** Under extreme misspecification (`extremeCHI`), the SG model may exhibit bias in the intercept ($\\mu$) as a mechanism to compensate for the constraints of the Centered Parameterization, prioritizing the accurate estimation of dynamics and dependence.
4.  **Computational Trade-off:** The flexibility of the SG model comes at a computational cost, evidenced by frequent MCMC issues. However, the statistical inference for core parameters appears robust to these issues.

## 10\. Exporting Key Tables

We save the processed and aggregated dataframes as CSV files for external analysis.

```{r export_tables}
#| label: export_tables

# 1. Main Condition-Level Summary (Aggregated across all successful runs)
# This table includes all metrics (Bias, Coverage, SD-Bias, RMSE, Posterior SD, Divergences)
# aggregated by condition, model, and parameter.

export_cond <- cond |>
  # Select relevant columns and restore original factor values for clarity
  select(condition_id, Model, param, skew_level, direction, T, 
         rho = rho_val, VARset = VARset_val,
         N_valid, N_truth_avail, 
         mean_rel_bias, coverage_95, RMSE,
         mean_post_sd, emp_sd, sd_bias, 
         mean_n_div, prop_div, mean_rhat)

write_csv(export_cond, file.path(EXPORT_DIR, "analysis_summary_aggregated.csv"))
# message("Exported aggregated summary to: ", file.path(EXPORT_DIR, "analysis_summary_aggregated.csv"))


# 2. Status-Split Summary (Aggregated within Clean/Problematic groups)
# This table allows for analyzing the impact of MCMC diagnostics on performance.

export_status <- cond_status |>
    # Select relevant columns
  select(condition_id, Model, param, mcmc_status, 
         skew_level, direction, T, 
         rho = rho_val, VARset = VARset_val,
         N_valid, 
         mean_rel_bias, coverage_95, RMSE,
         mean_post_sd, emp_sd, sd_bias)

write_csv(export_status, file.path(EXPORT_DIR, "analysis_summary_status_split.csv"))
# message("Exported status-split summary to: ", file.path(EXPORT_DIR, "analysis_summary_status_split.csv"))

# 3. MCMC Health Summary (Counts)
mcmc_health_export <- mcmc_summary |>
  pivot_wider(names_from = mcmc_status, values_from = Count, values_fill = list(Count = 0)) |>
  arrange(Model, skew_level, T)

write_csv(mcmc_health_export, file.path(EXPORT_DIR, "analysis_mcmc_health_counts.csv"))
# message("Exported MCMC health counts to: ", file.path(EXPORT_DIR, "analysis_mcmc_health_counts.csv"))

```
